{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-02-23T07:41:27.107467Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "X_test = X_test[:200]\n",
    "y_test = y_test[:200]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, 784))\n",
    "X_test = scaler.transform(X_test.reshape(-1, 784)) # для подачи в нейронную сеть их необходимо преобразовать в одномерные векторы\n",
    "\n",
    "# One-hot encoding меток\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "class ActivationFunction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def activate(self, z):\n",
    "        return NotImplementedError\n",
    "\n",
    "    def derivative(self, z):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def activate(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def derivative(self, z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "class GELU(ActivationFunction):\n",
    "    def activate(self, z):\n",
    "        return 0.5 * z * (1 + np.tanh(np.sqrt(2/np.pi) * (z + 0.044715 * z**3)))\n",
    "\n",
    "    def derivative(self, z):\n",
    "        tanh = np.tanh(np.sqrt(2/np.pi) * (z + 0.044715 * z**3))\n",
    "        derivative = 0.5 * (1 + tanh) + 0.5 * z * (np.sqrt(2/np.pi) * (1 + 0.13403 * z**2) * (1 - tanh**2))\n",
    "        return derivative\n",
    "\n",
    "class Softmax(ActivationFunction):\n",
    "    def activate(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def derivative(self, z):\n",
    "        s = self.activate(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, layer_config):\n",
    "        self.layers = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        prev_size = input_size\n",
    "        for config in layer_config:\n",
    "            activation = config['activation']()\n",
    "            self.layers.append({\n",
    "                'size': config['size'],\n",
    "                'activation': activation,\n",
    "                'z': None,\n",
    "                'a': None\n",
    "            })\n",
    "            current_size = config['size']\n",
    "            self.weights.append(np.random.randn(prev_size, current_size) * 0.01)\n",
    "            self.biases.append(np.zeros((1, current_size)))\n",
    "            prev_size = current_size\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        a = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = layer['activation'].activate(z)\n",
    "            layer['z'] = z\n",
    "            layer['a'] = a\n",
    "        return a\n",
    "\n",
    "    def backward_propagation(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        dz = (self.layers[-1]['a'] - y) / m\n",
    "\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            dW = np.dot(self.layers[i-1]['a'].T, dz) if i != 0 else np.dot(X.T, dz)\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            if i != 0:\n",
    "                dz = np.dot(dz, self.weights[i].T) * self.layers[i-1]['activation'].derivative(self.layers[i-1]['z'])\n",
    "            self.weights[i] -= learning_rate * dW\n",
    "            self.biases[i] -= learning_rate * db\n",
    "\n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward_propagation(X)\n",
    "            self.backward_propagation(X, y, learning_rate)\n",
    "            loss = self.hinge_loss(y, output)\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "            print(f'Accuracy {self.accuracy(y_test, self.predict(X_test))}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward_propagation(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        return np.mean(y_true == y_pred)\n",
    "\n",
    "    def hinge_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        return np.sum(np.maximum(0, 1 - y_true * y_pred)) / m\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "layer_config = [\n",
    "    {'size': 128, 'activation': ReLU},\n",
    "    {'size': 64, 'activation': GELU},\n",
    "    {'size': 32, 'activation': Softmax} \n",
    "]\n",
    "\n",
    "nn = NeuralNetwork(input_size, layer_config)\n",
    "nn.train(X_train, y_train_encoded, learning_rate=0.4, epochs=500)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd4620aeb5591600"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
